{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    },
    "colab": {
      "name": "connectx.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQcg2cwI0e_c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install 'kaggle-environments>=0.1.6'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "1_pIP8NP0e_r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import math\n",
        "import copy\n",
        "import numpy as np\n",
        "import gym\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "from kaggle_environments import evaluate, make\n",
        "from gym.spaces import Discrete\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kBWUDG70fAA",
        "colab_type": "text"
      },
      "source": [
        "### Short notes on gym.Env\n",
        "\n",
        "Attributes:\n",
        "+ action_space: The Space object corresponding to valid actions\n",
        "+ observation_space: The Space object corresponding to valid observations\n",
        "+ reward_range: A tuple corresponding to the min and max possible rewards\n",
        "\n",
        "Main methods:\n",
        "+ step()\n",
        "+ reset()\n",
        "+ render()\n",
        "+ close()\n",
        "+ seed()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3EEVINa0fAC",
        "colab_type": "text"
      },
      "source": [
        "# Connect-X Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "_NMWSqk00fAD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ConnectX(gym.Env):\n",
        "    def __init__(self):\n",
        "        super(ConnectX, self).__init__()\n",
        "        self.env = make('connectx', debug=False)\n",
        "        self.pair = [None, 'random']\n",
        "        self.side = 0\n",
        "        self.trainer = self.env.train(self.pair)\n",
        "        \n",
        "        config = self.env.configuration\n",
        "        self.action_space = gym.spaces.Discrete(config.columns) \n",
        "        self.observation_space = gym.spaces.Discrete(config.columns * config.rows + 1)\n",
        "    \n",
        "    def step(self, action):\n",
        "        obs, rew, done, info = self.trainer.step(action)\n",
        "        obs = obs['board'] + [obs['mark']]\n",
        "        return (obs, rew, done, info)\n",
        "    \n",
        "    def reset(self):\n",
        "        obs = self.trainer.reset()\n",
        "        return obs['board'] + [obs['mark']] # TODO: Maybe also append 'mark'.  see: self.action_space\n",
        "    \n",
        "    def render(self, **kwargs):\n",
        "        return self.env.render(**kwargs)\n",
        "    \n",
        "    def change_opponent(self, agent2):\n",
        "        self.pair = [None, agent2]\n",
        "        \n",
        "    def get_side(self):\n",
        "        return self.side\n",
        "    \n",
        "    def switch_side(self):\n",
        "        self.pair = self.pair[::-1]\n",
        "        self.trainer = self.env.train(self.pair)\n",
        "        self.side = 1 - self.side % 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FXJQvl40fAK",
        "colab_type": "text"
      },
      "source": [
        "# DQN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gc6Fv8ay0fAL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Replay:\n",
        "    def __init__(self, capacity=1e4):\n",
        "        self.capacity = capacity\n",
        "        self.buffer = []\n",
        "        self.position = 0\n",
        "        \n",
        "#     def load_replay(self):\n",
        "#         raise NotImplementedError\n",
        "    \n",
        "    def add_experience(self, experience):\n",
        "        \"\"\"\n",
        "        \n",
        "        Parameters:\n",
        "        experience ([np.array]): Experience which should be stored in the replay buffer\n",
        "        \"\"\"\n",
        "        if len(self.buffer) < self.capacity:\n",
        "            self.buffer.append(None)\n",
        "        self.buffer[int(self.position)] = experience\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "    \n",
        "    def get_batch(self, batch_size):\n",
        "        sample = random.sample(self.buffer, batch_size)\n",
        "        \n",
        "        obs_batch, act_batch, rew_batch, next_obs_batch, dones_batch = [], [], [], [], []\n",
        "        \n",
        "        for s in sample:\n",
        "            obs_batch.append(s[0])\n",
        "            act_batch.append(s[1])\n",
        "            rew_batch.append(s[2])\n",
        "            next_obs_batch.append(s[3])\n",
        "            dones_batch.append(s[4])\n",
        "        obs_batch = torch.tensor(obs_batch, dtype=torch.float32)\n",
        "        act_batch = torch.tensor(act_batch, dtype=torch.int64)\n",
        "        rew_batch = torch.tensor(rew_batch, dtype=torch.float32)\n",
        "        next_obs_batch = torch.tensor(next_obs_batch, dtype=torch.float32)\n",
        "        dones_batch = torch.tensor(dones_batch, dtype=torch.float32)\n",
        "        \n",
        "        # num_entries = len(sample[0])\n",
        "        \n",
        "        # batch = []\n",
        "        # for i in range(num_entries):\n",
        "        #     batch_i = []\n",
        "        #     for s in sample:\n",
        "        #         batch_i.append(s[i])\n",
        "        #     batch.append(batch_i)\n",
        "        \n",
        "        # torch_batch = [torch.tensor(b) for b in batch]\n",
        "        return (obs_batch, act_batch, rew_batch, next_obs_batch, dones_batch)\n",
        "    \n",
        "    def get_size(self):\n",
        "        return len(self.buffer)\n",
        "    \n",
        "    \n",
        "class QNet(nn.Module):\n",
        "    def __init__(self, env, n_hidden_layers=2, n_neurons=64):\n",
        "        super(QNet, self).__init__()\n",
        "        \n",
        "        self.action_space = env.action_space.n if isinstance(env.action_space, Discrete) \\\n",
        "            else env.action_space.shape[0]\n",
        "        self.observation_space = env.observation_space.n if isinstance(env.observation_space, Discrete) \\\n",
        "            else env.observation_space.shape[0]\n",
        "        \n",
        "        sizes = [self.observation_space] + [n_neurons for _ in range(n_hidden_layers)] + [self.action_space]\n",
        "        layers = []\n",
        "        for i in range(n_hidden_layers + 1):\n",
        "            layers.append(nn.Linear(sizes[i], sizes[i + 1]))\n",
        "            if i  < n_hidden_layers - 2:\n",
        "                layers.append(nn.ReLU())\n",
        "        self.layers = nn.Sequential(*layers)\n",
        "    \n",
        "    def forward(self, obs):\n",
        "        return self.layers(obs)\n",
        "    \n",
        "    #def get_action(self, obs, epsilon=0.9, deterministic=False):\n",
        "    #    '''\n",
        "    #    choose action based on epsilon-greedy.\n",
        "    #    '''\n",
        "    #    if random.random() < epsilon or deterministic:\n",
        "    #        return self.forward(obs).detach().argmax().item()\n",
        "    #    else:\n",
        "    #        return random.randint(0, self.action_space - 1)\n",
        "        \n",
        "\n",
        "class DQN:\n",
        "    def __init__(self, env, seed=0,\n",
        "                 replay_capacity=1e5, batch_size=100,\n",
        "                 gamma=0.99, tau=0.995, eps_start=0.9, eps_decay=200, eps_end=0.05,\n",
        "                 lr=1e-3, n_hidden_layers=2, n_neurons=64,**args):\n",
        "        \n",
        "        self.env = env\n",
        "        \n",
        "        self.action_space = env.action_space.n if isinstance(env.action_space, Discrete) \\\n",
        "            else env.action_space.shape[0]\n",
        "        self.observation_space = env.observation_space.n if isinstance(env.observation_space, Discrete) \\\n",
        "            else env.observation_space.shape[0]\n",
        "        \n",
        "        self.seed = seed\n",
        "        # self.replay = Replay(replay_capacity)\n",
        "        self.batch_size = batch_size\n",
        "        self.gamma = gamma\n",
        "        self.tau = tau\n",
        "        \n",
        "        self.eps = eps_start\n",
        "        self.eps_end = eps_end\n",
        "        self.eps_decay = eps_decay\n",
        "        \n",
        "        # initialize model and target model\n",
        "        self.critic = QNet(self.env, n_hidden_layers=n_hidden_layers, n_neurons=n_neurons)\n",
        "        self.target_critic = QNet(self.env, n_hidden_layers=n_hidden_layers, n_neurons=n_neurons)\n",
        "        for target_params in self.target_critic.parameters():\n",
        "            target_params.requires_grad = False\n",
        "        # copy parameter from critic to target_critic\n",
        "        self._soft_update(tau=0)\n",
        "#         for target_params, params in zip(self.target_critic.parameters(), self.critic.parameters()):\n",
        "#             target_params.data.copy_(params.data)\n",
        "        self.optimizer = Adam(self.critic.parameters(), lr=lr)\n",
        "    \n",
        "        self.loss = nn.MSELoss()\n",
        "    \n",
        "    def train(self, replay):\n",
        "        obs, act, rew, next_obs, dones = replay.get_batch(self.batch_size)       \n",
        "        \n",
        "        act = act.unsqueeze(1)\n",
        "        y = self.critic(obs).gather(1, act).squeeze()\n",
        "        \n",
        "        next_state_action_values = torch.max(self.target_critic(next_obs), 1).values.squeeze()\n",
        "        target = rew + self.gamma * (1 - dones) * next_state_action_values\n",
        "        \n",
        "        # optimize Critic\n",
        "        self.optimizer.zero_grad()\n",
        "        # loss = self.loss(y, target)\n",
        "        loss = ((y - target)**2).mean()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        \n",
        "        # Soft update\n",
        "        # self._soft_update()\n",
        "        \n",
        "        return loss.detach()\n",
        "        \n",
        "    def evaluate(self):\n",
        "        raise NotImplementedError\n",
        "        \n",
        "    def get_action(self, obs, deterministic=False):\n",
        "        '''\n",
        "        choose action based on epsilon-greedy.\n",
        "        '''\n",
        "        \n",
        "        if random.random() > self.eps or deterministic:\n",
        "            # disallow all actions where a stone is already placed\n",
        "            state_value = self.critic.forward(obs).detach()\n",
        "            for i in range(self.env.action_space.n):\n",
        "                if obs[i] != 0:\n",
        "                    state_value[i] = -1e7\n",
        "            return state_value.argmax().item()\n",
        "        \n",
        "        else:\n",
        "            return random.choice([a for a in range(self.env.action_space.n) if obs[a] == 0])\n",
        "    \n",
        "    def _soft_update(self, tau=None):\n",
        "        polyak = tau if tau is not None else self.tau\n",
        "        with torch.no_grad():\n",
        "            for target_p, p in zip(self.target_critic.parameters(), self.critic.parameters()):\n",
        "                target_p.data.mul_(polyak)\n",
        "                target_p.data.add_((1 - polyak) * p.data)\n",
        "    \n",
        "    def _update_eps(self):\n",
        "        self.eps = self.eps_end * (1 - math.exp(-1/self.eps_decay)) + self.eps * math.exp(-1/self.eps_decay)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAOcohGf0fAT",
        "colab_type": "text"
      },
      "source": [
        "# Multi-Agent Reinforcement Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dY5mXeYb0fAU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def play_game(env, model, replay=None, side=None, render=False, deterministic=False, train=False):\n",
        "    if side is None:\n",
        "        if random.randint(0,1):\n",
        "            env.switch_side()\n",
        "    elif side == 0:\n",
        "        if env.get_side() == 1:\n",
        "            env.switch_side()\n",
        "    elif side == 1: \n",
        "        if env.get_side() == 0:\n",
        "            env.switch_side()\n",
        "    # print('side: ', env.get_side())\n",
        "    rewards = []\n",
        "    cum_reward = 0\n",
        "    done = False\n",
        "    obs = env.reset()\n",
        "    obs = torch.tensor(obs).float()\n",
        "    # print(obs)\n",
        "    while not done:\n",
        "        action = model.get_action(obs, deterministic=deterministic)\n",
        "        next_obs, rew, done, _ = env.step(action)\n",
        "        next_obs = torch.tensor(next_obs).float()\n",
        "        # print(obs[:-1].view(6, -1))\n",
        "        # print(rew)\n",
        "        \n",
        "        if rew is None:\n",
        "            rew = 0\n",
        "        if done:\n",
        "            if rew == 1:\n",
        "                rew = 20\n",
        "            elif rew == -1:\n",
        "                rew = -20\n",
        "            else:\n",
        "                rew = 10\n",
        "        else:\n",
        "            rew = 0.5\n",
        "        \n",
        "        cum_reward += rew\n",
        "        rewards.append(rew)\n",
        "        \n",
        "        # add experience to replay buffer\n",
        "        if replay is not None:\n",
        "            replay.add_experience((obs.numpy(), action, rew, next_obs.numpy(), done))\n",
        "        if train:\n",
        "            loss = model.train(replay)\n",
        "        obs = next_obs\n",
        "    if render:\n",
        "        env.render(mode=\"ipython\", width=500, height=450)\n",
        "    \n",
        "    if replay is None:\n",
        "        return cum_reward\n",
        "    else:\n",
        "        if train:\n",
        "            return replay, loss\n",
        "        return replay\n",
        "    \n",
        "    \n",
        "def train_model(env, model, replay, epochs=100, min_buffer_size=1000, **args):\n",
        "    buffer_size = replay.get_size()\n",
        "    rewards = []\n",
        "    while replay.get_size() < min_buffer_size:\n",
        "        replay = play_game(env, model, replay)\n",
        "    for i in range(epochs):\n",
        "        replay, loss = play_game(env, model, replay, train=True)\n",
        "        # loss = model.train(replay)\n",
        "        model._update_eps()\n",
        "        if (i % 25) == 0:\n",
        "            model._soft_update(tau=0)\n",
        "        if ((i + 1) % 10) == 0:\n",
        "            cum_rewards_array = []\n",
        "            for _ in range(25):\n",
        "                cum_reward = play_game(env, model, deterministic=True)\n",
        "                cum_rewards_array.append(cum_reward)\n",
        "            print('Episode: ', i + 1, '\\tCritic loss: ', loss.item(), '\\tMean cumulated reward: ', np.array(cum_rewards_array).mean())\n",
        "            rewards.append(np.array(cum_rewards_array).mean())\n",
        "    return rewards"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FkN0wlI0fAa",
        "colab_type": "text"
      },
      "source": [
        "# Initialize training/Hyperparameter setting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9X11HNha0fAb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hyperparameter = {'lr': 1e-4,\n",
        "                  'replay_capacity': 1e6,\n",
        "                  'batch_size': 128,\n",
        "                  'gamma': 0.99,\n",
        "                  'tau': 0.995,\n",
        "                  'eps_start': 0.9,\n",
        "                  'eps_decay': 200, \n",
        "                  'eps_end': 0.05,\n",
        "                  'epochs': 10000,\n",
        "                  'min_buffer_size': 1000,\n",
        "                  'n_hidden_layers': 4,\n",
        "                  'n_neurons': 64\n",
        "                 }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGiSzKJV0fAg",
        "colab_type": "text"
      },
      "source": [
        "# Train Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "lfAHSTHx0fAh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = ConnectX()\n",
        "# env.change_opponent('negamax')\n",
        "replay = Replay()\n",
        "dqn = DQN(env, **hyperparameter)\n",
        "\n",
        "rewards = train_model(env, dqn, replay, **hyperparameter)\n",
        "\n",
        "print('Side selection: {}'.format(env.get_side()))\n",
        "play_game(env, dqn, render=True, side=0, deterministic=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThgTLVFD0fAn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure()\n",
        "plt.plot(rewards)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "IF62RMx20fAu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "play_game(env, dqn, render=True, deterministic=True)\n",
        "# print('Side', env.get_side(), env.pair)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVPF6a0v0fBB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env_play = make(\"connectx\", debug=True)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}