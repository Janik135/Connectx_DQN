{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "colab": {
   "name": "connectx.ipynb",
   "provenance": [],
   "private_outputs": true,
   "collapsed_sections": []
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "HQcg2cwI0e_c",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "!pip install 'kaggle-environments>=0.1.6'"
   ],
   "execution_count": 80,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kaggle-environments>=0.1.6 in ./venv/lib/python3.8/site-packages (1.0.13)\r\n",
      "Requirement already satisfied: jsonschema>=3.0.1 in ./venv/lib/python3.8/site-packages (from kaggle-environments>=0.1.6) (3.2.0)\r\n",
      "Requirement already satisfied: attrs>=17.4.0 in ./venv/lib/python3.8/site-packages (from jsonschema>=3.0.1->kaggle-environments>=0.1.6) (19.3.0)\r\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in ./venv/lib/python3.8/site-packages (from jsonschema>=3.0.1->kaggle-environments>=0.1.6) (0.16.0)\r\n",
      "Requirement already satisfied: setuptools in ./venv/lib/python3.8/site-packages (from jsonschema>=3.0.1->kaggle-environments>=0.1.6) (49.2.0)\r\n",
      "Requirement already satisfied: six>=1.11.0 in ./venv/lib/python3.8/site-packages (from jsonschema>=3.0.1->kaggle-environments>=0.1.6) (1.15.0)\r\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1_pIP8NP0e_r",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "import random\n",
    "import math\n",
    "import copy\n",
    "import numpy as np\n",
    "import gym\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from kaggle_environments import evaluate, make\n",
    "from gym.spaces import Discrete\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ],
   "execution_count": 81,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0kBWUDG70fAA",
    "colab_type": "text"
   },
   "source": [
    "### Short notes on gym.Env\n",
    "\n",
    "Attributes:\n",
    "+ action_space: The Space object corresponding to valid actions\n",
    "+ observation_space: The Space object corresponding to valid observations\n",
    "+ reward_range: A tuple corresponding to the min and max possible rewards\n",
    "\n",
    "Main methods:\n",
    "+ step()\n",
    "+ reset()\n",
    "+ render()\n",
    "+ close()\n",
    "+ seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w3EEVINa0fAC",
    "colab_type": "text"
   },
   "source": [
    "# Connect-X Environment"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_NMWSqk00fAD",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "class ConnectX(gym.Env):\n",
    "    def __init__(self, config):\n",
    "        super(ConnectX, self).__init__()\n",
    "        self.env = make('connectx', debug=False, configuration=config)\n",
    "        self.pair = [None, 'random']\n",
    "        self.side = 0\n",
    "        self.trainer = self.env.train(self.pair)\n",
    "        \n",
    "        config = self.env.configuration\n",
    "        self.action_space = gym.spaces.Discrete(config.columns) \n",
    "        self.observation_space = gym.spaces.Discrete(config.columns * config.rows + 1)\n",
    "    \n",
    "    def step(self, action):\n",
    "        obs, rew, done, info = self.trainer.step(action)\n",
    "        obs = obs['board'] + [obs['mark']]\n",
    "        return (obs, rew, done, info)\n",
    "    \n",
    "    def reset(self):\n",
    "        obs = self.trainer.reset()\n",
    "        return obs['board'] + [obs['mark']] # TODO: Maybe also append 'mark'.  see: self.action_space\n",
    "    \n",
    "    def render(self, **kwargs):\n",
    "        return self.env.render(**kwargs)\n",
    "    \n",
    "    def change_opponent(self, agent2):\n",
    "        self.pair = [None, agent2]\n",
    "        \n",
    "    def get_side(self):\n",
    "        return self.side\n",
    "    \n",
    "    def switch_side(self):\n",
    "        self.pair = self.pair[::-1]\n",
    "        self.trainer = self.env.train(self.pair)\n",
    "        self.side = 1 - self.side % 2"
   ],
   "execution_count": 82,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_FXJQvl40fAK",
    "colab_type": "text"
   },
   "source": [
    "# DQN"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "gc6Fv8ay0fAL",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "class Replay:\n",
    "    def __init__(self, capacity=1e4):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "        \n",
    "#     def load_replay(self):\n",
    "#         raise NotImplementedError\n",
    "    \n",
    "    def add_experience(self, experience):\n",
    "        \"\"\"\n",
    "        \n",
    "        Parameters:\n",
    "        experience ([np.array]): Experience which should be stored in the replay buffer\n",
    "        \"\"\"\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[int(self.position)] = experience\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "    \n",
    "    def get_batch(self, batch_size):\n",
    "        sample = random.sample(self.buffer, batch_size)\n",
    "        \n",
    "        obs_batch, act_batch, rew_batch, next_obs_batch, dones_batch = [], [], [], [], []\n",
    "        \n",
    "        for s in sample:\n",
    "            obs_batch.append(s[0])\n",
    "            act_batch.append(s[1])\n",
    "            rew_batch.append(s[2])\n",
    "            next_obs_batch.append(s[3])\n",
    "            dones_batch.append(s[4])\n",
    "        obs_batch = torch.tensor(obs_batch, dtype=torch.float32)\n",
    "        act_batch = torch.tensor(act_batch, dtype=torch.int64)\n",
    "        rew_batch = torch.tensor(rew_batch, dtype=torch.float32)\n",
    "        next_obs_batch = torch.tensor(next_obs_batch, dtype=torch.float32)\n",
    "        dones_batch = torch.tensor(dones_batch, dtype=torch.float32)\n",
    "        \n",
    "        # num_entries = len(sample[0])\n",
    "        \n",
    "        # batch = []\n",
    "        # for i in range(num_entries):\n",
    "        #     batch_i = []\n",
    "        #     for s in sample:\n",
    "        #         batch_i.append(s[i])\n",
    "        #     batch.append(batch_i)\n",
    "        \n",
    "        # torch_batch = [torch.tensor(b) for b in batch]\n",
    "        return (obs_batch, act_batch, rew_batch, next_obs_batch, dones_batch)\n",
    "    \n",
    "    def get_size(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    \n",
    "class QNet(nn.Module):\n",
    "    def __init__(self, env, n_conv_layers=1, n_lin_layers=2, n_neurons=64):\n",
    "        super(QNet, self).__init__()\n",
    "        \n",
    "        self.action_space = env.action_space.n if isinstance(env.action_space, Discrete) \\\n",
    "            else env.action_space.shape[0]\n",
    "        self.observation_space = env.observation_space.n if isinstance(env.observation_space, Discrete) \\\n",
    "            else env.observation_space.shape[0]\n",
    "\n",
    "        sizes = [self.observation_space] + [n_neurons for _ in range(n_lin_layers)] + [self.action_space]\n",
    "        layers = []\n",
    "        layers.append(nn.Conv1d(1, 3, 3, padding=1))\n",
    "        layers.append(nn.ReLU())\n",
    "        for i in range(n_lin_layers + 1):\n",
    "            layers.append(nn.Linear(sizes[i], sizes[i + 1]))\n",
    "            if i  < n_lin_layers:\n",
    "                layers.append(nn.ReLU())\n",
    "\n",
    "        # with convolution\n",
    "        # layers = []\n",
    "        # layers.append(nn.Conv1d(1, n_neurons, 3, padding=1))\n",
    "        # layers.append(nn.ReLU())\n",
    "        # sizes = [n_neurons for _ in range(n_lin_layers)] + [self.action_space]\n",
    "        # for i in range(n_lin_layers):\n",
    "        #     layers.append(nn.Linear(sizes[i], sizes[i + 1]))\n",
    "        #     if i  < n_lin_layers-1:\n",
    "        #         layers.append(nn.ReLU())\n",
    "\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        obs = obs.unsqueeze(1)\n",
    "        return self.layers(obs).squeeze()\n",
    "    \n",
    "    #def get_action(self, obs, epsilon=0.9, deterministic=False):\n",
    "    #    '''\n",
    "    #    choose action based on epsilon-greedy.\n",
    "    #    '''\n",
    "    #    if random.random() < epsilon or deterministic:\n",
    "    #        return self.forward(obs).detach().argmax().item()\n",
    "    #    else:\n",
    "    #        return random.randint(0, self.action_space - 1)\n",
    "        \n",
    "\n",
    "class DQN:\n",
    "    def __init__(self, env, seed=0,\n",
    "                 replay_capacity=1e5, batch_size=100,\n",
    "                 gamma=0.99, tau=0.995, eps_start=0.9, eps_decay=200, eps_end=0.05,\n",
    "                 lr=1e-3, n_hidden_layers=2, n_neurons=64,**args):\n",
    "        \n",
    "        self.env = env\n",
    "        \n",
    "        self.action_space = env.action_space.n if isinstance(env.action_space, Discrete) \\\n",
    "            else env.action_space.shape[0]\n",
    "        self.observation_space = env.observation_space.n if isinstance(env.observation_space, Discrete) \\\n",
    "            else env.observation_space.shape[0]\n",
    "        \n",
    "        self.seed = seed\n",
    "        # self.replay = Replay(replay_capacity)\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        \n",
    "        self.eps = eps_start\n",
    "        self.eps_end = eps_end\n",
    "        self.eps_decay = eps_decay\n",
    "        \n",
    "        # initialize model and target model\n",
    "        self.critic = QNet(self.env, n_lin_layers=n_hidden_layers, n_neurons=n_neurons)\n",
    "        self.target_critic = QNet(self.env, n_lin_layers=n_hidden_layers, n_neurons=n_neurons)\n",
    "        for target_params in self.target_critic.parameters():\n",
    "            target_params.requires_grad = False\n",
    "        # copy parameter from critic to target_critic\n",
    "        self._soft_update(tau=0)\n",
    "#         for target_params, params in zip(self.target_critic.parameters(), self.critic.parameters()):\n",
    "#             target_params.data.copy_(params.data)\n",
    "        self.optimizer = Adam(self.critic.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "        self.loss = nn.MSELoss()\n",
    "    \n",
    "    def train(self, replay):\n",
    "        obs, act, rew, next_obs, dones = replay.get_batch(self.batch_size)       \n",
    "\n",
    "\n",
    "        act = act.unsqueeze(1)\n",
    "        y = self.critic(obs).gather(1, act).squeeze()\n",
    "        \n",
    "        next_state_action_values = torch.max(self.target_critic(next_obs), 1).values.squeeze()\n",
    "        target = rew + self.gamma * (1 - dones) * next_state_action_values\n",
    "        \n",
    "        # optimize Critic\n",
    "        self.optimizer.zero_grad()\n",
    "        # loss = self.loss(y, target)\n",
    "        loss = ((y - target)**2).mean()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Soft update\n",
    "        # self._soft_update()\n",
    "        \n",
    "        return loss.detach()\n",
    "        \n",
    "    def evaluate(self):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def get_action(self, obs, deterministic=False):\n",
    "        '''\n",
    "        choose action based on epsilon-greedy.\n",
    "        '''\n",
    "        \n",
    "        if random.random() > self.eps or deterministic:\n",
    "            # disallow all actions where a stone is already placed\n",
    "            state_value = self.critic.forward(obs).detach()\n",
    "            for i in range(self.env.action_space.n):\n",
    "                if obs[i] != 0:\n",
    "                    state_value[i] = -1e7\n",
    "            return state_value.argmax().item()\n",
    "        \n",
    "        else:\n",
    "            return random.choice([a for a in range(self.env.action_space.n) if obs[a] == 0])\n",
    "    \n",
    "    def _soft_update(self, tau=None):\n",
    "        polyak = tau if tau is not None else self.tau\n",
    "        with torch.no_grad():\n",
    "            for target_p, p in zip(self.target_critic.parameters(), self.critic.parameters()):\n",
    "                target_p.data.mul_(polyak)\n",
    "                target_p.data.add_((1 - polyak) * p.data)\n",
    "    \n",
    "    def _update_eps(self):\n",
    "        self.eps = self.eps_end * (1 - math.exp(-1/self.eps_decay)) + self.eps * math.exp(-1/self.eps_decay)"
   ],
   "execution_count": 83,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nAOcohGf0fAT",
    "colab_type": "text"
   },
   "source": [
    "# Multi-Agent Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "dY5mXeYb0fAU",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "def play_game(env, model, replay=None, side=None, render=False, deterministic=False, train=False):\n",
    "    if side is None:\n",
    "        if random.randint(0,1):\n",
    "            env.switch_side()\n",
    "    elif side == 0:\n",
    "        if env.get_side() == 1:\n",
    "            env.switch_side()\n",
    "    elif side == 1: \n",
    "        if env.get_side() == 0:\n",
    "            env.switch_side()\n",
    "    # print('side: ', env.get_side())\n",
    "    rewards = []\n",
    "    cum_reward = 0\n",
    "    done = False\n",
    "    obs = env.reset()\n",
    "    obs = torch.tensor(obs).float()\n",
    "    # print(obs)\n",
    "    while not done:\n",
    "        action = model.get_action(obs, deterministic=deterministic)\n",
    "        next_obs, rew, done, _ = env.step(action)\n",
    "        next_obs = torch.tensor(next_obs).float()\n",
    "        # print(obs[:-1].view(6, -1))\n",
    "        # print(rew)\n",
    "        \n",
    "        if rew is None:\n",
    "            rew = 0\n",
    "        if done:\n",
    "            if rew == 1:\n",
    "                rew = 20\n",
    "            elif rew == -1:\n",
    "                rew = -20\n",
    "            else:\n",
    "                rew = 10\n",
    "        else:\n",
    "            rew = 0.5\n",
    "        \n",
    "        cum_reward += rew\n",
    "        rewards.append(rew)\n",
    "        \n",
    "        # add experience to replay buffer\n",
    "        if replay is not None:\n",
    "            replay.add_experience((obs.numpy(), action, rew, next_obs.numpy(), done))\n",
    "        if train:\n",
    "            loss = model.train(replay)\n",
    "        obs = next_obs\n",
    "    if render:\n",
    "        env.render(mode=\"ipython\", width=500, height=450)\n",
    "    \n",
    "    if replay is None:\n",
    "        return cum_reward\n",
    "    else:\n",
    "        if train:\n",
    "            return replay, loss\n",
    "        return replay\n",
    "    \n",
    "    \n",
    "def train_model(env, model, replay, epochs=100, min_buffer_size=1000, **args):\n",
    "    buffer_size = replay.get_size()\n",
    "    rewards = []\n",
    "    while replay.get_size() < min_buffer_size:\n",
    "        replay = play_game(env, model, replay)\n",
    "    for i in range(epochs):\n",
    "        replay, loss = play_game(env, model, replay, train=True)\n",
    "        # loss = model.train(replay)\n",
    "        model._update_eps()\n",
    "        if (i % 25) == 0:\n",
    "            model._soft_update(tau=0)\n",
    "        if ((i + 1) % 10) == 0:\n",
    "            cum_rewards_array = []\n",
    "            for _ in range(25):\n",
    "                cum_reward = play_game(env, model, deterministic=True, side=0)\n",
    "                cum_rewards_array.append(cum_reward)\n",
    "            print('Episode: ', i + 1, '\\tCritic loss: ', loss.item(), '\\tMean cumulated reward: ', np.array(cum_rewards_array).mean(), '\\tEpsilon:', model.eps)\n",
    "            rewards.append(np.array(cum_rewards_array).mean())\n",
    "    return rewards"
   ],
   "execution_count": 84,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0FkN0wlI0fAa",
    "colab_type": "text"
   },
   "source": [
    "# Initialize training/Hyperparameter setting"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9X11HNha0fAb",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "hyperparameter = {'lr': 1e-3,\n",
    "                  'replay_capacity': 1e6,\n",
    "                  'batch_size': 256,\n",
    "                  'gamma': 0.99,\n",
    "                  'tau': 0.995,\n",
    "                  'eps_start': 1.,\n",
    "                  'eps_decay': 2000,\n",
    "                  'eps_end': 0.05,\n",
    "                  'epochs': 15000,\n",
    "                  'min_buffer_size': 1000,\n",
    "                  'n_hidden_layers': 2,\n",
    "                  'n_neurons': 64\n",
    "                 }"
   ],
   "execution_count": 85,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VGiSzKJV0fAg",
    "colab_type": "text"
   },
   "source": [
    "# Train Agent"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "lfAHSTHx0fAh",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "t0 = time.time()\n",
    "conf = {'rows': 6, 'columns': 7, 'inarow': 4}\n",
    "env = ConnectX(config=conf)\n",
    "env.change_opponent('negamax')\n",
    "replay = Replay()\n",
    "dqn = DQN(env, **hyperparameter)\n",
    "\n",
    "rewards = train_model(env, dqn, replay, **hyperparameter)\n",
    "t1 = time.time()\n",
    "print(f\"Trained {hyperparameter['epochs']} episodes in {t1-t0:.2f} seconds\")\n",
    "\n",
    "print('Side selection: {}'.format(env.get_side()))\n",
    "play_game(env, dqn, render=True, side=0, deterministic=True)"
   ],
   "execution_count": 86,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Index tensor must have the same number of dimensions as input tensor",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-86-ad71723039c1>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[0mdqn\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mDQN\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0menv\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mhyperparameter\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 8\u001B[0;31m \u001B[0mrewards\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtrain_model\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0menv\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdqn\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreplay\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mhyperparameter\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      9\u001B[0m \u001B[0mt1\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtime\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf\"Trained {hyperparameter['epochs']} episodes in {t1-t0:.2f} seconds\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-84-29cc7ab748ff>\u001B[0m in \u001B[0;36mtrain_model\u001B[0;34m(env, model, replay, epochs, min_buffer_size, **args)\u001B[0m\n\u001B[1;32m     61\u001B[0m         \u001B[0mreplay\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mplay_game\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0menv\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreplay\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     62\u001B[0m     \u001B[0;32mfor\u001B[0m \u001B[0mi\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mepochs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 63\u001B[0;31m         \u001B[0mreplay\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mloss\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mplay_game\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0menv\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreplay\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrain\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     64\u001B[0m         \u001B[0;31m# loss = model.train(replay)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     65\u001B[0m         \u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_update_eps\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-84-29cc7ab748ff>\u001B[0m in \u001B[0;36mplay_game\u001B[0;34m(env, model, replay, side, render, deterministic, train)\u001B[0m\n\u001B[1;32m     42\u001B[0m             \u001B[0mreplay\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0madd_experience\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mobs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnumpy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maction\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrew\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnext_obs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnumpy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     43\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mtrain\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 44\u001B[0;31m             \u001B[0mloss\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mreplay\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     45\u001B[0m         \u001B[0mobs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnext_obs\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     46\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mrender\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-83-831c3064ba09>\u001B[0m in \u001B[0;36mtrain\u001B[0;34m(self, replay)\u001B[0m\n\u001B[1;32m    138\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    139\u001B[0m         \u001B[0mact\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mact\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0munsqueeze\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 140\u001B[0;31m         \u001B[0my\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcritic\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mobs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgather\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mact\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msqueeze\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    141\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    142\u001B[0m         \u001B[0mnext_state_action_values\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmax\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtarget_critic\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnext_obs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msqueeze\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Index tensor must have the same number of dimensions as input tensor"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ThgTLVFD0fAn",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "plt.figure()\n",
    "plt.plot(rewards)\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": false,
    "id": "IF62RMx20fAu",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "play_game(env, dqn, render=True, deterministic=True)\n",
    "# print('Side', env.get_side(), env.pair)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "mVPF6a0v0fBB",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "env_play = make(\"connectx\", debug=True)"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}